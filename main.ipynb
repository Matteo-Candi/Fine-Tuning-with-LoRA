{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Matteo-Candi/Fine-Tuning-with-LoRA/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiO3kDvPiOjL"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fDJc2ZzR7PV2"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nlxrr9nB7ZkF"
      },
      "outputs": [],
      "source": [
        "from transformers import set_seed, BertTokenizer, TFBertModel, TFBertForSequenceClassification, TFBertMainLayer\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "import tensorflow as tf\n",
        "\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uklyioe8gCB"
      },
      "outputs": [],
      "source": [
        "# To ensure reproducible results (as much as possible)\n",
        "tf.keras.utils.set_random_seed(1234)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GeP3g_39qff"
      },
      "source": [
        "# Dataset and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJ1Ohapq4bGY"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"go_emotions\", \"simplified\").shuffle(seed=17)\n",
        "labels = dataset['train'].features['labels'].feature.names\n",
        "dataset = dataset.remove_columns('id')\n",
        "\n",
        "\n",
        "def filter_labels(example):\n",
        "    return len(example['labels']) < 2\n",
        "\n",
        "for split in dataset.keys():\n",
        "    dataset[split] = dataset[split].filter(filter_labels)\n",
        "\n",
        "model_name = \"bert-base-cased\"\n",
        "num_labels = 28\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGU6UiKZ-1ns"
      },
      "outputs": [],
      "source": [
        "def get_df_summary(df):\n",
        "    label_column = df['labels'].to_list()\n",
        "    flatten_col = list(itertools.chain(*label_column))\n",
        "    freq_dict = dict(Counter(flatten_col))\n",
        "    freq_list = list(dict(sorted(freq_dict.items())).values())\n",
        "    rel_freq_list = [el / sum(freq_list) * 100 for el in freq_list]\n",
        "\n",
        "    return rel_freq_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_RZkkj-9B9k"
      },
      "outputs": [],
      "source": [
        "df_train = dataset['train'].to_pandas()\n",
        "df_val = dataset['validation'].to_pandas()\n",
        "df_test = dataset['test'].to_pandas()\n",
        "\n",
        "# Get dataframes info.\n",
        "train_freq = get_df_summary(df_train)\n",
        "val_freq = get_df_summary(df_val)\n",
        "test_freq = get_df_summary(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ihLCE6P6ts-"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(15, 6))\n",
        "bar_width = 0.25\n",
        "index = np.arange(len(train_freq))\n",
        "bars_train = ax.barh(index, train_freq, bar_width, label='Train')\n",
        "bars_val = ax.barh(index + bar_width, val_freq, bar_width, label='Validation')\n",
        "bars_test = ax.barh(index + 2*bar_width, test_freq, bar_width, label='Test')\n",
        "plt.xlabel('%')\n",
        "plt.title('Percentage of Feelings')\n",
        "plt.yticks(index + bar_width, labels)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hyPxnsy3jP5"
      },
      "outputs": [],
      "source": [
        "def tokenizer_func(text):\n",
        "    '''\n",
        "    Tokenizing the text using the BERT tokenizer.\n",
        "    '''\n",
        "    # Checking there is just one tokenization up to 316 and the second longest one is 120.\n",
        "    # token_lengths = [sum(tokenized_train[i][0] != 0) for i in range(len(tokenized_train))]\n",
        "    return tokenizer(text, padding = 'max_length', max_length = 120, return_tensors = \"tf\", truncation=True)\n",
        "\n",
        "def one_hot_func(indices):\n",
        "    '''\n",
        "    Converting a value in one hot encode vector.\n",
        "    '''\n",
        "    one_hot_encoding = np.zeros(28, dtype=int)\n",
        "    one_hot_encoding[indices] = 1\n",
        "    return {'labels': one_hot_encoding}\n",
        "\n",
        "def process_dataset_func(dataset):\n",
        "    '''\n",
        "    Processing the dataset to make it readable from the model.\n",
        "    '''\n",
        "    processed_dataset = dataset.map(lambda x: tokenizer_func(x['text']))\n",
        "    processed_dataset = processed_dataset.map(lambda x: one_hot_func(x['labels']))\n",
        "    processed_dataset = processed_dataset.remove_columns('text')\n",
        "    return processed_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5PxvE7A2M1j"
      },
      "outputs": [],
      "source": [
        "dataset = process_dataset_func(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7dtM6KA-X6h"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "tf_dataset_train = tf.data.Dataset.from_tensor_slices((dataset['train']['input_ids'], dataset['train']['labels'])).batch(batch_size)\n",
        "tf_dataset_validation = tf.data.Dataset.from_tensor_slices((dataset['validation']['input_ids'], dataset['validation']['labels'])).batch(batch_size)\n",
        "tf_dataset_test = tf.data.Dataset.from_tensor_slices((dataset['test']['input_ids'], dataset['test']['labels'])).batch(batch_size)\n",
        "\n",
        "# Reshape labels to match the model output.\n",
        "tf_dataset_train = tf_dataset_train.map(lambda input_ids, labels: (tf.squeeze(input_ids, axis=1), labels))\n",
        "tf_dataset_validation = tf_dataset_validation.map(lambda input_ids, labels: (tf.squeeze(input_ids, axis=1), labels))\n",
        "tf_dataset_test = tf_dataset_test.map(lambda input_ids, labels: (tf.squeeze(input_ids, axis=1), labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLMlpFElAk9x"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRePUVqATjag"
      },
      "source": [
        "## Base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xvx24Qi1Aqws",
        "outputId": "05aba272-5ba6-4132-f93b-3cc5a08c8708"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"tf_bert_for_sequence_classification_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bert (TFBertMainLayer)      multiple                  108310272 \n",
            "                                                                 \n",
            " dropout_242 (Dropout)       multiple                  0 (unused)\n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  21532     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 108331804 (413.25 MB)\n",
            "Trainable params: 108331804 (413.25 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "pretrained_model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels = num_labels)\n",
        "pretrained_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1LjNjkG0q6I"
      },
      "outputs": [],
      "source": [
        "class BaseModel(tf.keras.Model):\n",
        "    def __init__(self, model_name, num_labels):\n",
        "        super(BaseModel, self).__init__()\n",
        "        self.num_labels = num_labels\n",
        "\n",
        "        # When importing the model some weigths have different initialization states.\n",
        "        set_seed(17)\n",
        "        self.model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels = self.num_labels)\n",
        "        self.softmax = tf.keras.layers.Activation('softmax')\n",
        "\n",
        "\n",
        "    def call(self, input):\n",
        "\n",
        "        # The model detect automatically 'input_ids' and 'attention_mask'.\n",
        "        logits = self.model(input)[0]\n",
        "        model_output = self.softmax(logits)\n",
        "\n",
        "\n",
        "        return model_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop"
      ],
      "metadata": {
        "id": "IXZ-yMeAAjKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ij7yXURwMd-3",
        "outputId": "be342d8e-a0ff-4810-fea3-1ae05b894b22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1135/1135 [==============================] - 978s 806ms/step - loss: 1.6934 - accuracy: 0.5419 - val_loss: 1.3562 - val_accuracy: 0.6102\n",
            "Epoch 2/5\n",
            "1135/1135 [==============================] - 913s 804ms/step - loss: 1.0871 - accuracy: 0.6727 - val_loss: 1.3531 - val_accuracy: 0.6025\n",
            "Epoch 3/5\n",
            "1135/1135 [==============================] - 914s 805ms/step - loss: 0.6283 - accuracy: 0.8094 - val_loss: 1.6252 - val_accuracy: 0.5772\n",
            "Epoch 4/5\n",
            "1135/1135 [==============================] - 895s 789ms/step - loss: 0.3395 - accuracy: 0.8971 - val_loss: 1.8789 - val_accuracy: 0.5851\n",
            "Epoch 5/5\n",
            "1135/1135 [==============================] - 892s 786ms/step - loss: 0.1989 - accuracy: 0.9394 - val_loss: 2.1411 - val_accuracy: 0.5772\n"
          ]
        }
      ],
      "source": [
        "base_model = BaseModel(model_name, num_labels)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
        "metrics = ['accuracy']\n",
        "\n",
        "base_model.compile(optimizer=optimizer,\n",
        "              loss=loss_function,\n",
        "              metrics=metrics)\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "base_model_history = base_model.fit(tf_dataset_train, validation_data=tf_dataset_validation, epochs=num_epochs, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f5452f2-0105-4bb4-fbe5-546c021d2068",
        "id": "bj7pKNTlBSTf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [1.6934291124343872,\n",
              "  1.0871013402938843,\n",
              "  0.6282956004142761,\n",
              "  0.3395499289035797,\n",
              "  0.19886009395122528],\n",
              " 'accuracy': [0.5418640375137329,\n",
              "  0.6727442741394043,\n",
              "  0.809380829334259,\n",
              "  0.8971301317214966,\n",
              "  0.939434826374054],\n",
              " 'val_loss': [1.3562015295028687,\n",
              "  1.3530546426773071,\n",
              "  1.6251964569091797,\n",
              "  1.878911018371582,\n",
              "  2.1410646438598633],\n",
              " 'val_accuracy': [0.6101583242416382,\n",
              "  0.602462649345398,\n",
              "  0.5771768093109131,\n",
              "  0.5850923657417297,\n",
              "  0.5771768093109131]}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "base_model_history.history\n",
        "\n",
        "# {'loss': [1.6934291124343872,\n",
        "#   1.0871013402938843,\n",
        "#   0.6282956004142761,\n",
        "#   0.3395499289035797,\n",
        "#   0.19886009395122528],\n",
        "#  'accuracy': [0.5418640375137329,\n",
        "#   0.6727442741394043,\n",
        "#   0.809380829334259,\n",
        "#   0.8971301317214966,\n",
        "#   0.939434826374054],\n",
        "#  'val_loss': [1.3562015295028687,\n",
        "#   1.3530546426773071,\n",
        "#   1.6251964569091797,\n",
        "#   1.878911018371582,\n",
        "#   2.1410646438598633],\n",
        "#  'val_accuracy': [0.6101583242416382,\n",
        "#   0.602462649345398,\n",
        "#   0.5771768093109131,\n",
        "#   0.5850923657417297,\n",
        "#   0.5771768093109131]}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data ={'loss': [1.6934291124343872,\n",
        "  1.0871013402938843,\n",
        "  0.6282956004142761,\n",
        "  0.3395499289035797,\n",
        "  0.19886009395122528],\n",
        " 'accuracy': [0.5418640375137329,\n",
        "  0.6727442741394043,\n",
        "  0.809380829334259,\n",
        "  0.8971301317214966,\n",
        "  0.939434826374054],\n",
        " 'val_loss': [1.3562015295028687,\n",
        "  1.3530546426773071,\n",
        "  1.6251964569091797,\n",
        "  1.878911018371582,\n",
        "  2.1410646438598633],\n",
        " 'val_accuracy': [0.6101583242416382,\n",
        "  0.602462649345398,\n",
        "  0.5771768093109131,\n",
        "  0.5850923657417297,\n",
        "  0.5771768093109131]}"
      ],
      "metadata": {
        "id": "nsQrkpjVIAp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(data['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "U9I_wXpcH-mt",
        "outputId": "4a161742-c5ce-4616-9680-9b8e57e468d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7e6f2d486050>]"
            ]
          },
          "metadata": {},
          "execution_count": 248
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = base_model.evaluate(tf_dataset_test)\n",
        "accuracy_on_test_set = output[1]\n",
        "round(accuracy_on_test_set, 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e68a299e-9750-48b6-8169-c317cf6ed979",
        "id": "L-z4keWACmiN"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5863"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4kO71wNTo79"
      },
      "source": [
        "## Model fine-tuned with LoRA"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LoraLayer(tf.keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        original_layer,\n",
        "        rank=8,\n",
        "        alpha=32,\n",
        "        **kwargs):\n",
        "\n",
        "        super().__init__(**kwargs)\n",
        "        original_layer_config = original_layer.get_config()\n",
        "        name = original_layer_config[\"name\"]\n",
        "\n",
        "        self.rank = rank\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self._scale = alpha / rank\n",
        "\n",
        "        self._hidden_dim = original_layer_config['units']\n",
        "\n",
        "        self.original_layer = tf.keras.layers.Dense(self._hidden_dim, name=f'lora_{name}')\n",
        "        self.original_layer.build(self._hidden_dim)\n",
        "        self.original_layer.set_weights(original_layer.get_weights())\n",
        "\n",
        "\n",
        "        # LoRA dense layers.\n",
        "        self.A = tf.keras.layers.Dense(\n",
        "            units=rank,\n",
        "            use_bias=False,\n",
        "            # Note: the original paper mentions that normal distribution was\n",
        "            # used for initialization. However, the official LoRA implementation\n",
        "            # uses \"Kaiming/He Initialization\".\n",
        "            kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
        "                scale=math.sqrt(5), mode=\"fan_in\", distribution=\"uniform\", seed=17\n",
        "            ),\n",
        "            name=\"lora_A\",\n",
        "        )\n",
        "\n",
        "        self.B = tf.keras.layers.EinsumDense(\n",
        "            equation='abc,cd->abd',\n",
        "            output_shape=(120, 768),\n",
        "            kernel_initializer=\"zeros\",\n",
        "            name=\"lora_B\",\n",
        "        )\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        original_output = self.original_layer(inputs)\n",
        "            # If we are fine-tuning the model, we will add LoRA layers' output\n",
        "            # to the original layer's output.\n",
        "        A_output = self.A(inputs)\n",
        "        B_output = self.B(A_output)\n",
        "        lora_output = B_output * self._scale\n",
        "\n",
        "        return lora_output + original_output\n",
        "\n",
        "\n",
        "\n",
        "class LoraModel(tf.keras.Model):\n",
        "    def __init__(self, model_name, num_labels, rank, alpha, model):\n",
        "        super(LoraModel, self).__init__()\n",
        "\n",
        "        # When importing the model some weigths have different initialization states.\n",
        "        # self.model = model\n",
        "        set_seed(17)\n",
        "        self.model = model\n",
        "        # for layer in self.model.bert._flatten_layers():\n",
        "        #     layer.trainable = False\n",
        "\n",
        "        # self.add_lora_layers_to_bert(rank, alpha)\n",
        "        # self.model.bert.trainable = False\n",
        "\n",
        "        self.softmax = tf.keras.layers.Activation('softmax')\n",
        "\n",
        "\n",
        "    def add_lora_layers_to_bert(self, rank=8, alpha=32):\n",
        "\n",
        "        for i in range(12):\n",
        "\n",
        "            self.model.bert.encoder.layer[i].attention.self_attention.query = LoraLayer(\n",
        "                self.model.bert.encoder.layer[i].attention.self_attention.query,\n",
        "                rank=rank,\n",
        "                alpha=alpha)\n",
        "\n",
        "            self.model.bert.encoder.layer[i].attention.self_attention.key = LoraLayer(\n",
        "                self.model.bert.encoder.layer[i].attention.self_attention.key,\n",
        "                rank=rank,\n",
        "                alpha=alpha)\n",
        "\n",
        "            self.model.bert.encoder.layer[i].attention.self_attention.value = LoraLayer(\n",
        "                    self.model.bert.encoder.layer[i].attention.self_attention.value,\n",
        "                    rank=rank,\n",
        "                    alpha=alpha)\n",
        "\n",
        "\n",
        "\n",
        "    def call(self, input):\n",
        "\n",
        "        # The model detect automatically 'input_ids' and 'attention_mask'.\n",
        "        logits = self.model(input)[0]\n",
        "        model_output = self.softmax(logits)\n",
        "\n",
        "        return model_output"
      ],
      "metadata": {
        "id": "aRySG4gpqP_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_lora_layers_to_bert(rank=8, alpha=32):\n",
        "    for i in range(12):\n",
        "\n",
        "        model.bert.encoder.layer[i].attention.self_attention.query = LoraLayer(\n",
        "            model.bert.encoder.layer[i].attention.self_attention.query,\n",
        "            rank=rank,\n",
        "            alpha=alpha)\n",
        "\n",
        "        model.bert.encoder.layer[i].attention.self_attention.key = LoraLayer(\n",
        "            model.bert.encoder.layer[i].attention.self_attention.key,\n",
        "            rank=rank,\n",
        "            alpha=alpha)\n",
        "\n",
        "        model.bert.encoder.layer[i].attention.self_attention.value = LoraLayer(\n",
        "                model.bert.encoder.layer[i].attention.self_attention.value,\n",
        "                rank=rank,\n",
        "                alpha=alpha)"
      ],
      "metadata": {
        "id": "LKLzgSh-W-d8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-cased', num_labels = 28)\n",
        "add_lora_layers_to_bert(8, 16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXL9pwYOW22v",
        "outputId": "d03f6333-b862-4d90-96d4-4f761a0923f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in list(model._flatten_modules()):\n",
        "    if len(list(layer._flatten_modules())) == 1:\n",
        "        if layer.name not in ['lora_A', 'lora_B', 'classifier']:\n",
        "            layer.trainable = False"
      ],
      "metadata": {
        "id": "wyIkLZ_AbkXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.bert.encoder.layer[i].intermediate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBSxyshfy1AZ",
        "outputId": "186f4b5e-8a4b-461c-98a3-330eca7a0253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<transformers.models.bert.modeling_tf_bert.TFBertIntermediate at 0x7ec9dc676410>"
            ]
          },
          "metadata": {},
          "execution_count": 441
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.bert.embeddings.LayerNorm.trainable = False\n",
        "model.bert.pooler.dense.trainable = False\n",
        "\n",
        "for i in range(len(model.bert.encoder.layer)):\n",
        "    layer = model.bert.encoder.layer[i]\n",
        "\n",
        "    layer.bert_output.LayerNorm.trainable = False\n",
        "    layer.bert_output.dense.trainable = False\n",
        "\n",
        "    layer.intermediate.dense.trainable = False\n",
        "\n",
        "    layer.attention.dense_output.dense.trainable = False\n",
        "    layer.attention.dense_output.LayerNorm.trainable = False\n",
        "\n",
        "    layer.attention.self_attention.query.original_layer.trainable = False\n",
        "    layer.attention.self_attention.key.original_layer.trainable = False\n",
        "    layer.attention.self_attention.value.original_layer.trainable = False"
      ],
      "metadata": {
        "id": "Tln9UvMbv1LI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_element = next(iter(tf_dataset_train))[0]\n",
        "_ = model(first_element)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifWRq2UlXJst",
        "outputId": "0a659c86-6b44-4bb9-9f3d-d05ea1c63752"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tf_bert_for_sequence_classification_74\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bert (TFBertMainLayer)      multiple                  108752640 \n",
            "                                                                 \n",
            " dropout_2849 (Dropout)      multiple                  0         \n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  21532     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 108774172 (414.94 MB)\n",
            "Trainable params: 23127580 (88.22 MB)\n",
            "Non-trainable params: 85646592 (326.72 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(23127580 - 463900) / 12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6m-6DK-rx0ge",
        "outputId": "3b5f7f9b-ea3e-4721-8f7b-703a2c8c775f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.1978760294755095"
            ]
          },
          "metadata": {},
          "execution_count": 456
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "108774172 - 108331804"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSayvlMr0FJS",
        "outputId": "6b028b26-f475-499e-cd49-2483a5e86913"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "442368"
            ]
          },
          "metadata": {},
          "execution_count": 459
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameteres for LoRA.\n",
        "rank = 8\n",
        "alpha = 16\n",
        "\n",
        "# Create the model.\n",
        "lora_model = LoraModel(model_name, num_labels, rank, alpha, model)\n",
        "\n",
        "# Build the model passing a sample through it.\n",
        "first_element = next(iter(tf_dataset_train))[0]\n",
        "_ = lora_model(first_element)"
      ],
      "metadata": {
        "id": "8uWhmhH40WoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start considering that we want to implement LoRA layers in all the matrices if the self attention (#3) for all the heads of bert-base (#12).\n",
        "\n",
        "The total number of Query, Key and Value matrices' parameters is $21261312$,\n",
        "\n",
        "$21261312 / 12 = 1771776$ parameters for each head,\n",
        "\n",
        "$1771776 / 3 = 590592 = 768 \\times 768 + 768$ parameters for each matrix.\n",
        "\n",
        "\n",
        "The total number of new parameters add with LoRA are $(768 \\times rank + rank \\times 768) \\times 3 \\times 12 = 442368$.\n",
        "\n",
        "So, considering also the classifier's parameters, the total amount of trainable parameters is $442368 + 21532 = 463900$."
      ],
      "metadata": {
        "id": "QPc3gDa90WoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.AdamW(learning_rate=5e-4)\n",
        "loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
        "metrics = ['accuracy']\n",
        "\n",
        "lora_model.compile(optimizer=optimizer,\n",
        "              loss=loss_function,\n",
        "              metrics=metrics)\n",
        "\n",
        "num_epochs = 1\n",
        "\n",
        "lora_model_history = lora_model.fit(tf_dataset_train, validation_data=tf_dataset_validation, epochs=num_epochs, batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "b2e958d0-c12e-4b13-eb87-2a5c2dbb3bd2",
        "id": "lZQ2tRXD0WoB"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  14/1135 [..............................] - ETA: 14:53 - loss: 2.8617 - accuracy: 0.3103"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-117-49d41bd651b9>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mlora_model_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlora_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_dataset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf_dataset_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lora_model.model.classifier.get_weights()[0]"
      ],
      "metadata": {
        "outputId": "589d632f-63d8-42be-afc6-052f83bfcedb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfKltgcs0WoC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.00591019, -0.00484577,  0.01100573, ..., -0.00311598,\n",
              "        -0.02416917,  0.00409354],\n",
              "       [-0.02039267,  0.00942053,  0.0025434 , ...,  0.00891148,\n",
              "         0.0012288 ,  0.00054411],\n",
              "       [ 0.00893728, -0.02101945, -0.00922329, ...,  0.007544  ,\n",
              "         0.00898259,  0.03755317],\n",
              "       ...,\n",
              "       [-0.00194424,  0.00653094, -0.0041351 , ..., -0.01534467,\n",
              "        -0.02725969,  0.01637444],\n",
              "       [ 0.01192218,  0.00274544, -0.01700132, ..., -0.00941994,\n",
              "        -0.02123295,  0.01323891],\n",
              "       [ 0.00884701, -0.00757739,  0.00854274, ...,  0.00947676,\n",
              "        -0.01690845,  0.02813397]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 260
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lora_model.model.bert.encoder.layer[11].attention.self_attention.key.A.get_weights()"
      ],
      "metadata": {
        "outputId": "1d410584-cfe8-4103-b30b-29454ac02bb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QT7k28z0WoC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[ 0.0884343 , -0.07185138,  0.05943378, ..., -0.08395838,\n",
              "          0.02477893,  0.02181597],\n",
              "        [-0.00091249, -0.08824681,  0.06573258, ..., -0.06960507,\n",
              "         -0.08878529,  0.05152037],\n",
              "        [-0.06314621, -0.0055666 , -0.05208993, ..., -0.08678542,\n",
              "          0.01738612, -0.05438538],\n",
              "        ...,\n",
              "        [ 0.0630657 , -0.0043783 , -0.01258984, ..., -0.08278234,\n",
              "         -0.02204023, -0.06064276],\n",
              "        [ 0.04710453,  0.01308293,  0.0593427 , ..., -0.04989512,\n",
              "         -0.00659908,  0.07868818],\n",
              "        [ 0.0525265 ,  0.00650924,  0.01982998, ..., -0.00834987,\n",
              "          0.07899635,  0.01037245]], dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lora_model.model.bert.encoder.layer[0].attention.self_attention.key.original_layer.get_weights()[0]"
      ],
      "metadata": {
        "outputId": "7893cc1b-ccd9-4845-81c9-26a50e1155e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdDeY78y0WoC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2.8657176e-02,  4.8005264e-03,  4.9075916e-02, ...,\n",
              "        -8.8107968e-03,  5.6885206e-03,  5.4868605e-02],\n",
              "       [-3.0682154e-02, -3.0306482e-03, -1.2872593e-02, ...,\n",
              "        -3.2314227e-03,  5.2937653e-02, -3.6515070e-03],\n",
              "       [-2.6920761e-03,  2.4164479e-02, -1.3647523e-02, ...,\n",
              "         1.1721017e-02,  9.4898082e-03,  1.5124909e-02],\n",
              "       ...,\n",
              "       [ 2.3737766e-02,  7.4703549e-03,  3.8690038e-02, ...,\n",
              "        -2.8807104e-02,  5.6113109e-02, -2.0291412e-02],\n",
              "       [-7.9208333e-03,  4.4420104e-02,  1.6604092e-02, ...,\n",
              "         4.6373899e-03,  2.6797490e-02, -2.8913473e-03],\n",
              "       [-1.0335554e-02,  2.5628829e-02,  7.9800091e-05, ...,\n",
              "         3.5981752e-02,  4.9961805e-02,  1.4802449e-02]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lora_model_history.history"
      ],
      "metadata": {
        "id": "xgGmtJc8FZIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg7jBMSbFEHC"
      },
      "source": [
        "Sources:\n",
        "- [Multi class with Bert](https://github.com/theartificialguy/NLP-with-Deep-Learning/blob/master/BERT/Multi-Class%20classification%20TF-BERT/multi_class.ipynb)\n",
        "\n",
        "- [Dataset Preprocessing](https://huggingface.co/docs/datasets/v1.12.1/use_dataset.html)\n",
        "\n",
        "- [Bert model layers comprehension](https://github.com/huggingface/transformers/blob/ef609958586a24b7943ad6c31184ff5a84b6f8e2/src/transformers/models/bert/modeling_tf_bert.py#L369)\n",
        "\n",
        "- [LoRA Layer and injection](https://github.com/keras-team/keras-io/blob/master/examples/nlp/parameter_efficient_finetuning_of_gpt2_with_lora.py)\n",
        "\n",
        "- [LoRA parameters](https://arxiv.org/abs/2106.09685)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KPvO2_qr1v_v"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}